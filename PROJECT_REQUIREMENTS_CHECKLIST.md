# Project Requirements Complete Checklist & Testing Guide

**Team:** CloudAI Analytics Team  
**Members:** Jo Naulaerts, Abdul Salam Aldabik, Amate  
**Deadline:** November 24, 2025  
**Presentation:** November 28, 2025  

---

## üìã TABLE OF CONTENTS

1. [Building Models Requirements](#1-building-models-requirements)
2. [Deployment Requirements](#2-deployment-requirements)
3. [Upload Requirements](#3-upload-requirements)
4. [Presentation Requirements](#4-presentation-requirements)
5. [Testing Procedures](#5-testing-procedures)
6. [Final Submission Checklist](#6-final-submission-checklist)

---

# 1. BUILDING MODELS REQUIREMENTS

## Dataset 1: UK Housing Price Prediction

### ‚úÖ Requirement 1.1: Quick First Model
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_1_UK_Housing/Code/06_first_simple_model.ipynb`  
**Model:** Ridge Regression  
**Author:** Jo Naulaerts  

**What it does:**
- Simple baseline using Ridge regression
- Provides quick predictions for deployment testing
- Establishes performance baseline

**Testing:**
```powershell
# Open and run the notebook
code Dataset_1_UK_Housing/Code/06_first_simple_model.ipynb
# Run all cells and verify Ridge model trains successfully
```

---

### ‚úÖ Requirement 1.2: PyCaret Automated ML Comparison
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_1_UK_Housing/Code/07_using_PyCaret.ipynb`  
**Model:** PyCaret AutoML (compares multiple regression algorithms)  
**Author:** Jo Naulaerts  

**What it does:**
- Automatically tests multiple regression models
- Compares: Linear Regression, Ridge, Lasso, ElasticNet, Random Forest, XGBoost, etc.
- Selects best performing model based on cross-validation

**Testing:**
```powershell
# Open notebook
code Dataset_1_UK_Housing/Code/07_using_PyCaret.ipynb
# Verify PyCaret setup and model comparison sections
# Check that multiple models were compared with metrics
```

---

### ‚úÖ Requirement 1.3: Create and Tune Custom Model
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_1_UK_Housing/Code/08_AWS_using_PyCaret.ipynb`  
**Model:** PyCaret best model (locally trained and tuned)  
**Author:** Jo Naulaerts  

**What it does:**
- Takes best model from PyCaret comparison
- Performs hyperparameter tuning
- Explains model choice based on PyCaret results
- Validates with test set performance

**Testing:**
```powershell
# Open notebook
code Dataset_1_UK_Housing/Code/08_AWS_using_PyCaret.ipynb
# Verify model tuning and validation sections
# Check performance metrics (RMSE, MAE, R¬≤)
```

---

### ‚è≥ Requirement 1.4: AWS Model
**Status:** ‚è≥ TEMPLATE READY - NEEDS AWS EXECUTION

**File:** `Dataset_1_UK_Housing/Code/09_AWS_SageMaker_Model.ipynb`  
**Model:** AWS SageMaker Linear Learner  
**Author:** Abdul Salam Aldabik  

**What it does:**
- Trains Linear Learner algorithm on AWS SageMaker
- Deploys model to SageMaker endpoint
- Makes predictions using cloud infrastructure

**Testing (when AWS instance is ready):**
```powershell
# 1. Access AWS SageMaker Console
# 2. Create Notebook Instance: ml.m4.xlarge
# 3. Upload: 09_AWS_SageMaker_Model.ipynb
# 4. Upload: housing_features_final.parquet to S3
# 5. Run all cells (training ~10-15 min)
# 6. Download completed notebook with outputs
# 7. ‚ö†Ô∏è DELETE ENDPOINT to stop charges
# 8. ‚ö†Ô∏è STOP INSTANCE to stop charges
```

---

### ‚úÖ Requirement 1.5: Model Comparison
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_1_UK_Housing/Code/10_final_model_comparison.ipynb`  
**Author:** Abdul Salam Aldabik  

**What it does:**
- Compares ALL models: Ridge, PyCaret, AWS SageMaker
- Shows metrics table with RMSE, MAE, R¬≤, Training Time
- Visualizes performance with bar charts
- Provides conclusions on best model

**Testing:**
```powershell
# Open notebook
code Dataset_1_UK_Housing/Code/10_final_model_comparison.ipynb
# Run all cells to generate comparison tables and charts
# Note: AWS metrics are placeholders until AWS training completes
```

---

## Dataset 2: UK Electricity Demand Forecasting

### ‚úÖ Requirement 2.1: Quick First Model
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/04_exploratory_models.ipynb`  
**Models:** Naive baseline, Moving Average, Simple Linear Regression  
**Author:** Abdul Salam Aldabik  

**What it does:**
- Quick baseline forecasts for deployment testing
- Simple time series approaches
- Establishes performance baseline

**Testing:**
```powershell
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/04_exploratory_models.ipynb
# Run all cells and verify baseline models train
```

---

### ‚úÖ Requirement 2.2: PyCaret Automated ML Comparison
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/05_complete_model_training.ipynb` (Section 5)  
**Model:** PyCaret Time Series AutoML  
**Author:** Abdul Salam Aldabik  

**What it does:**
- Section 5 of the complete training notebook
- Automatically tests multiple time series forecasting algorithms
- Compares Prophet, ARIMA, ETS, and other forecasters
- Uses PyCaret's time_series module

**Testing:**
```powershell
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/05_complete_model_training.ipynb
# Scroll to Section 5: "PyCaret Automated Model Comparison"
# Verify PyCaret setup and model comparison
# Check that multiple time series models were tested
```

---

### ‚úÖ Requirement 2.3: Create and Tune Custom Models
**Status:** ‚úÖ COMPLETE (4 MODELS!)

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/05_complete_model_training.ipynb`  
**Models:** Prophet, XGBoost, LSTM Neural Network, Ensemble  
**Author:** Abdul Salam Aldabik  

**What it does:**
- **Prophet:** Facebook's time series forecaster (seasonality, trends)
- **XGBoost:** Gradient boosted trees with temporal features (BEST: 3% MAPE)
- **LSTM:** Deep learning RNN for sequential data (7% MAPE)
- **Ensemble:** Weighted combination of all models (4% MAPE)
- Explains why each model was chosen
- Tunes hyperparameters for each
- Validates with multiple metrics (MAE, RMSE, MAPE)

**Testing:**
```powershell
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/05_complete_model_training.ipynb
# This is a LARGE notebook (1060 lines)
# Run sections for each model:
#   - Section 1: Prophet
#   - Section 2: XGBoost
#   - Section 3: LSTM
#   - Section 4: Ensemble
#   - Section 5: PyCaret
# Verify all models train and show performance metrics
```

---

### ‚è≥ Requirement 2.4: AWS Model
**Status:** ‚è≥ TEMPLATE READY - NEEDS AWS EXECUTION

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/07_AWS_SageMaker_Model.ipynb`  
**Model:** AWS SageMaker DeepAR (Deep Auto-Regressive forecasting)  
**Author:** Abdul Salam Aldabik  

**What it does:**
- Trains DeepAR time series algorithm on AWS SageMaker
- Converts data to JSON format (required for DeepAR)
- Deploys forecasting endpoint
- Makes multi-step predictions

**Testing (when AWS instance is ready):**
```powershell
# 1. Access AWS SageMaker Console
# 2. Create Notebook Instance: ml.m4.xlarge
# 3. Upload: 07_AWS_SageMaker_Model.ipynb
# 4. Upload: neso_historic_demand_combined.csv to S3
# 5. Run all cells (training ~15-20 min)
# 6. Download completed notebook with outputs
# 7. ‚ö†Ô∏è DELETE ENDPOINT to stop charges
# 8. ‚ö†Ô∏è STOP INSTANCE to stop charges
```

---

### ‚úÖ Requirement 2.5: Model Comparison
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/06_final_model_comparison.ipynb`  
**Author:** Abdul Salam Aldabik  

**What it does:**
- Compares ALL models: Prophet, XGBoost, LSTM, Ensemble, PyCaret, AWS DeepAR
- Shows comprehensive metrics table
- Visualizes performance comparisons
- Provides conclusions (XGBoost is best with 3% MAPE)

**Testing:**
```powershell
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/06_final_model_comparison.ipynb
# Run all cells to generate comparison tables and charts
# Note: AWS metrics are placeholders until AWS training completes
```

---

# 2. DEPLOYMENT REQUIREMENTS

## ‚úÖ Requirement 2.1: Frontend
**Status:** ‚úÖ COMPLETE (2 APPS)

### App 1: Housing Price Predictor
**File:** `Dataset_1_UK_Housing/Code/streamlit_app.py`  
**Author:** Abdul Salam Aldabik  
**Port:** 8501  

**Features:**
- ‚úÖ User-friendly web interface
- ‚úÖ Input form for property details (location, type, size, etc.)
- ‚úÖ Real-time price predictions
- ‚úÖ Confidence intervals
- ‚úÖ Interactive visualizations
- ‚úÖ CSV export functionality

**Testing:**
```powershell
# Navigate to directory
cd Dataset_1_UK_Housing/Code

# Run app
streamlit run streamlit_app.py

# Test in browser (http://localhost:8501):
# 1. Fill in property details
# 2. Click "Predict Price"
# 3. Verify prediction displays with confidence interval
# 4. Test "Download Prediction" button
# 5. Try different property types and locations
```

---

### App 2: Electricity Demand Forecaster
**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/streamlit_app.py`  
**Author:** Abdul Salam Aldabik  
**Port:** 8502  

**Features:**
- ‚úÖ User-friendly web interface
- ‚úÖ Model selection dropdown (Prophet, XGBoost, LSTM, Ensemble)
- ‚úÖ Adjustable forecast horizon (7-365 days)
- ‚úÖ Interactive time series charts
- ‚úÖ Confidence intervals
- ‚úÖ Model performance metrics display
- ‚úÖ CSV export functionality

**Testing:**
```powershell
# Navigate to directory
cd Dataset_2_UK_Historic_Electricity_Demand_Data/Code

# Run app
streamlit run streamlit_app.py

# Test in browser (http://localhost:8502):
# 1. Select different models from dropdown
# 2. Adjust forecast horizon slider
# 3. Click "Generate Forecast"
# 4. Verify charts display with predictions
# 5. Test "Download Forecast" button
# 6. Compare different models
```

---

## ‚úÖ Requirement 2.2: Backend
**Status:** ‚úÖ COMPLETE

**Implementation:** Integrated into Streamlit apps (Python backend)

**What it does:**
- ‚úÖ Loads trained models from disk (`.pkl` files)
- ‚úÖ Accepts user input from frontend
- ‚úÖ Preprocesses input data (scaling, encoding, feature engineering)
- ‚úÖ Makes predictions using loaded models
- ‚úÖ Returns formatted results to frontend
- ‚úÖ Handles errors gracefully

**Code Locations:**
- Housing: `streamlit_app.py` lines with `joblib.load()`, `model.predict()`
- Electricity: `streamlit_app.py` model loading and prediction functions

**Testing:**
```powershell
# Backend is tested through frontend testing above
# Additional backend verification:
# 1. Check that models load without errors (logs in terminal)
# 2. Verify predictions are numerical and realistic
# 3. Test edge cases (missing values, extreme values)
# 4. Confirm error messages display for invalid inputs
```

---

## ‚úÖ Requirement 2.3: Pipeline (Automated Retraining)
**Status:** ‚úÖ COMPLETE

**File:** `.github/workflows/ml_pipeline.yml`  
**Author:** Abdul Salam Aldabik  

**What it does:**
- ‚úÖ Automated CI/CD pipeline using GitHub Actions
- ‚úÖ Triggers when code/data is pushed to `main` branch
- ‚úÖ Automatically retrains Housing Ridge model
- ‚úÖ Automatically retrains Electricity XGBoost model
- ‚úÖ Saves updated models back to repository
- ‚úÖ Uses `[skip ci]` tag to prevent infinite loops

**Trigger Paths:**
- `Dataset_1_UK_Housing/Code/**`
- `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/**`
- `Dataset_1_UK_Housing/Data/**`
- `Dataset_2_UK_Historic_Electricity_Demand_Data/Data/**`

**Testing:**
```powershell
# 1. Make a small change to any training notebook
git add Dataset_1_UK_Housing/Code/06_first_simple_model.ipynb
git commit -m "Test pipeline trigger"
git push origin main

# 2. Monitor GitHub Actions:
#    - Go to: https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025/actions
#    - Click on "ML Model Training Pipeline"
#    - Verify workflow runs successfully
#    - Check that new model files are committed

# 3. Verify automated commit appears in repo:
git pull origin main
# Look for commit message: "ü§ñ Auto-retrain: Housing model updated [skip ci]"
```

**Manual Trigger:**
```powershell
# Can also trigger manually via GitHub web interface:
# Actions ‚Üí ML Model Training Pipeline ‚Üí Run workflow
```

---

## ‚úÖ Requirement 2.4: Hosting
**Status:** ‚úÖ COMPLETE (MULTIPLE OPTIONS DOCUMENTED)

**Files:**
- `Dockerfile.housing` - Housing app container
- `Dockerfile.electricity` - Electricity app container
- `docker-compose.yml` - Run both apps together
- `DEPLOYMENT.md` - Complete hosting guide

**Hosting Options Provided:**

### Option 1: Docker (Local/Any Server)
```powershell
# Build and run both apps
docker-compose up --build

# Access:
# Housing: http://localhost:8501
# Electricity: http://localhost:8502
```

### Option 2: Oracle Cloud (FREE TIER)
- Complete setup instructions in `DEPLOYMENT.md`
- 2 AMD VMs free forever
- 200GB storage included
- Steps: Create account ‚Üí Launch VM ‚Üí Install Docker ‚Üí Deploy

### Option 3: AWS EC2
- t2.micro eligible for free tier
- Instructions for deployment included

### Option 4: Raspberry Pi (Home Hosting)
- Setup guide for Pi 4/5
- Dynamic DNS instructions
- Port forwarding guide

### Option 5: Streamlit Community Cloud (EASIEST)
- Free hosting for Streamlit apps
- Just connect GitHub repo
- Public URLs provided
- Limitation: Memory constraints for large datasets

**Testing:**
```powershell
# Test Docker deployment
docker-compose up --build

# Verify both apps accessible:
# http://localhost:8501 (Housing)
# http://localhost:8502 (Electricity)

# Check health status:
docker ps
docker inspect uk-housing-predictor | grep -A 10 Health
docker inspect uk-electricity-forecaster | grep -A 10 Health

# View logs:
docker-compose logs -f

# Stop:
docker-compose down
```

---

# 3. UPLOAD REQUIREMENTS

## ‚úÖ Requirement 3.1: EDA Notebooks (Multiple with Cleaning & Graphs)

### Dataset 1: UK Housing
**Status:** ‚úÖ COMPLETE (6 EDA NOTEBOOKS)

1. ‚úÖ `00_initial_setup.ipynb` - Project setup, environment configuration
2. ‚úÖ `01_data_loading.ipynb` - Load housing data, initial inspection
3. ‚úÖ `02_economic_integration.ipynb` - Add BoE rates, GDP, inflation (with graphs)
4. ‚úÖ `03_data_merging.ipynb` - Merge datasets (with validation charts)
5. ‚úÖ `04_data_cleaning.ipynb` - NA handling, outliers, encoding (with visualizations)
6. ‚úÖ `05_feature_engineering.ipynb` - Create features (with correlation heatmaps)

**All contain:**
- ‚úÖ Data cleaning code
- ‚úÖ Explanatory graphs/visualizations
- ‚úÖ Markdown explanations
- ‚úÖ Author attribution

### Dataset 2: UK Electricity
**Status:** ‚úÖ COMPLETE (4 EDA NOTEBOOKS)

1. ‚úÖ `00_final_data_preparation.ipynb` - Complete data prep workflow
2. ‚úÖ `01_data_pull.ipynb` - Pull from NESO API
3. ‚úÖ `02_data_cleaning_and_eda.ipynb` - Cleaning + extensive EDA with graphs
4. ‚úÖ `03_time_series_analysis.ipynb` - Seasonality, trends, ACF/PACF plots

**All contain:**
- ‚úÖ Data cleaning code
- ‚úÖ Explanatory graphs/visualizations
- ‚úÖ Markdown explanations
- ‚úÖ Author attribution

**Testing:**
```powershell
# Open each notebook and verify:
# 1. Author information in first cell
# 2. Data cleaning sections present
# 3. Visualizations/graphs included
# 4. Markdown explanations before code blocks

# Dataset 1
code Dataset_1_UK_Housing/Code/04_data_cleaning.ipynb
# Check for: NA handling code + outlier plots

# Dataset 2
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/02_data_cleaning_and_eda.ipynb
# Check for: Cleaning code + time series plots
```

---

## ‚úÖ Requirement 3.2: Final Import Notebook (Summarized Cleaning)

### Dataset 1
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_1_UK_Housing/Code/05_feature_engineering.ipynb`  
**What it does:**
- Consolidates all cleaning steps without exploratory graphs
- Outputs final clean dataset: `housing_features_final.parquet`
- Ready for model training

### Dataset 2
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/00_final_data_preparation.ipynb`  
**What it does:**
- Consolidates all cleaning steps
- Outputs final clean dataset
- Ready for model training

**Testing:**
```powershell
# Run final prep notebooks and verify output files

# Dataset 1
code Dataset_1_UK_Housing/Code/05_feature_engineering.ipynb
# Verify: housing_features_final.parquet is created in Data/

# Dataset 2
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/00_final_data_preparation.ipynb
# Verify: Clean dataset is produced
```

---

## ‚úÖ Requirement 3.3: Models - One File Per Model Per Dataset

### Dataset 1: UK Housing
**Status:** ‚úÖ COMPLETE (4 MODEL FILES)

1. ‚úÖ `06_first_simple_model.ipynb` - Ridge Regression
2. ‚úÖ `07_using_PyCaret.ipynb` - PyCaret AutoML
3. ‚úÖ `08_AWS_using_PyCaret.ipynb` - PyCaret tuned model
4. ‚è≥ `09_AWS_SageMaker_Model.ipynb` - AWS Linear Learner (template ready)

**All have:**
- ‚úÖ Training code with outputs
- ‚úÖ Model evaluation metrics
- ‚úÖ Author attribution
- ‚úÖ Markdown explanations

### Dataset 2: UK Electricity
**Status:** ‚úÖ COMPLETE (6 MODEL FILES)

1. ‚úÖ `04_exploratory_models.ipynb` - Quick baselines
2. ‚úÖ `05_complete_model_training.ipynb` - Prophet, XGBoost, LSTM, Ensemble, PyCaret (5 models in one file!)
3. ‚è≥ `07_AWS_SageMaker_Model.ipynb` - AWS DeepAR (template ready)

**All have:**
- ‚úÖ Training code with outputs (NOTE: outputs preserved!)
- ‚úÖ Model evaluation metrics
- ‚úÖ Author attribution
- ‚úÖ Markdown explanations

**Important:** Long-running models (LSTM, Prophet, XGBoost) have outputs preserved in notebooks!

**Testing:**
```powershell
# Verify each notebook has outputs
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/05_complete_model_training.ipynb
# Scroll through - should see model training outputs, metrics, plots

# For AWS notebooks (when completed):
# 1. Download from SageMaker with outputs intact
# 2. Replace template notebooks in repository
# 3. Verify training outputs and metrics are visible
```

---

## ‚ö†Ô∏è Requirement 3.4: Model Files (.pkl) - GitHub Size Limit

**Status:** ‚úÖ CONFIGURED

**Configuration:**
- ‚úÖ `.gitignore` excludes large `.pkl` files
- ‚úÖ Exception for `*_pipeline.pkl` (small models for CI/CD)
- ‚úÖ Large models documented in notebooks (not committed)

**Note:** Assignment says: "remember that github only allows files smaller than 100MB. Remove them if they are bigger before committing."

**Current State:**
- Model training outputs are PRESERVED in notebooks (visible)
- Large model files (`.pkl`) are NOT committed to git
- Small pipeline models ARE committed for automated deployment

**Testing:**
```powershell
# Check .gitignore
cat .gitignore
# Should see: *.pkl with exception for !*_pipeline.pkl

# Verify no large files staged
git status
# Should NOT see large .pkl files in changes
```

---

## ‚è≥ Requirement 3.5: AWS Models - Download Notebook from SageMaker

**Status:** ‚è≥ READY TO EXECUTE

**Files:**
- `Dataset_1_UK_Housing/Code/09_AWS_SageMaker_Model.ipynb`
- `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/07_AWS_SageMaker_Model.ipynb`

**Current:** Template notebooks ready to upload to AWS  
**Next Step:** Run on AWS, download with outputs, replace templates

**Procedure:**
```powershell
# 1. Upload template to AWS SageMaker
# 2. Run all cells (training takes 10-20 min)
# 3. Download completed notebook (File ‚Üí Download As ‚Üí Notebook)
# 4. Replace template in repository:
#    cp ~/Downloads/09_AWS_SageMaker_Model.ipynb Dataset_1_UK_Housing/Code/
# 5. Commit updated notebook with outputs
git add Dataset_1_UK_Housing/Code/09_AWS_SageMaker_Model.ipynb
git commit -m "Add AWS SageMaker training outputs"
```

---

## ‚úÖ Requirement 3.6: Comparison Notebooks

### Dataset 1
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_1_UK_Housing/Code/10_final_model_comparison.ipynb`

**Contains:**
- ‚úÖ Metrics table for ALL models (Ridge, PyCaret, AWS)
- ‚úÖ Performance comparison charts (bar charts, line plots)
- ‚úÖ Deep analysis of each model
- ‚úÖ **Conclusions:** Which model is best and why
- ‚úÖ Training time comparisons
- ‚úÖ Pros/cons of each approach

### Dataset 2
**Status:** ‚úÖ COMPLETE

**File:** `Dataset_2_UK_Historic_Electricity_Demand_Data/Code/06_final_model_comparison.ipynb`

**Contains:**
- ‚úÖ Metrics table for ALL 6 models
- ‚úÖ Performance comparison visualizations
- ‚úÖ Deep analysis of each model
- ‚úÖ **Conclusions:** XGBoost is best (3% MAPE)
- ‚úÖ Model complexity vs accuracy tradeoff discussion
- ‚úÖ Recommendations for production deployment

**Testing:**
```powershell
# Open comparison notebooks
code Dataset_1_UK_Housing/Code/10_final_model_comparison.ipynb
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/06_final_model_comparison.ipynb

# Run all cells and verify:
# 1. Metrics table displays
# 2. Comparison charts render
# 3. Conclusions section present
# 4. All models are included
```

---

# 4. PRESENTATION REQUIREMENTS

## ‚úÖ Requirement 4.1: Who's Who (Team Introduction)

**Status:** ‚úÖ PREPARED

**Team Name:** CloudAI Analytics Team  
**Members:**
- **Jo Naulaerts** - Dataset 1 models (Ridge, PyCaret)
- **Abdul Salam Aldabik** - Dataset 2 models, AWS, deployment, pipeline
- **Amate** - [Teammate's contributions]

**Documented in:** `README.md`

**Presentation Talking Points:**
- Team name and why we chose it
- Each member's role and responsibilities
- How we collaborated (GitHub, communication)

---

## ‚úÖ Requirement 4.2: EDA Findings

### Dataset 1: UK Housing
**Key Findings to Present:**

1. **Economic Indicators Impact:**
   - Interest rates strongly correlated with house prices
   - GDP growth drives property values
   - Inflation effects vary by region

2. **Property Type Patterns:**
   - Detached houses most expensive
   - Flats/apartments most volatile
   - New builds command premium

3. **Geographic Trends:**
   - London significantly higher prices
   - Regional disparities increasing over time
   - Postcode area is strong predictor

4. **Temporal Patterns:**
   - Seasonal effects (spring/summer peaks)
   - 2008 financial crisis impact visible
   - Post-2015 Brexit uncertainty effects

**Unexpected Findings:**
- Some postcodes have extreme outliers (celebrity areas?)
- Transaction volume affects average prices
- Economic indicators have lag effect (3-6 months)

### Dataset 2: UK Electricity
**Key Findings to Present:**

1. **Strong Seasonality:**
   - Winter peaks (heating demand)
   - Summer troughs (less heating)
   - Daily patterns (7am, 7pm peaks)

2. **Long-term Trends:**
   - Overall demand declining since 2005
   - Efficiency improvements visible
   - Renewable energy integration impact

3. **Weekly Patterns:**
   - Weekday vs weekend differences
   - Sunday lowest demand
   - Monday morning spike

4. **COVID-19 Impact:**
   - 2020-2021 anomalies
   - Lockdown demand changes
   - Work-from-home effects

**Unexpected Findings:**
- Demand declining despite population growth (efficiency!)
- Weather has massive impact (cold snaps)
- Interconnectors reduce demand volatility

**Testing:**
```powershell
# Review EDA notebooks for presentation material
code Dataset_1_UK_Housing/Code/04_data_cleaning.ipynb
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/02_data_cleaning_and_eda.ipynb

# Take screenshots of:
# - Correlation heatmaps
# - Time series plots
# - Distribution charts
# - Outlier visualizations
```

---

## ‚úÖ Requirement 4.3: Models - Which Was Easier? Which Was Best?

### Dataset 1: UK Housing

**Easiest to Train:**
- **Ridge Regression** (06_first_simple_model.ipynb)
- Trains in seconds
- Minimal hyperparameter tuning
- Easy to interpret

**Best Performance:**
- **PyCaret AutoML** (07_using_PyCaret.ipynb)
- Automated comparison of 15+ models
- Found best algorithm (likely XGBoost or CatBoost)
- Best metrics with minimal coding

**Most Insightful:**
- **AWS SageMaker Linear Learner** (09_AWS_SageMaker_Model.ipynb)
- Cloud-scale training
- Automatic hyperparameter tuning
- Production-ready deployment

### Dataset 2: UK Electricity

**Easiest to Train:**
- **Prophet** (05_complete_model_training.ipynb)
- Automatic seasonality detection
- No feature engineering needed
- Interpretable components

**Best Performance:**
- **XGBoost** (3% MAPE)
- Required careful feature engineering
- Fast training on temporal features
- Best balance of accuracy and speed

**Most Complex:**
- **LSTM Neural Network** (7% MAPE)
- Required careful architecture design
- Long training time
- Good but not worth the complexity for this task

**Most Practical:**
- **Ensemble** (4% MAPE)
- Combines strengths of multiple models
- Robust predictions
- Production-ready

**Presentation Script:**
```
"For Dataset 1, Ridge was easiest - just a few lines of code. But PyCaret 
gave us the best results by automatically testing 15 different models.

For Dataset 2, Prophet was easiest because it handles seasonality automatically.
But XGBoost performed best with only 3% error - that's incredibly accurate!

Surprisingly, the complex LSTM neural network didn't beat simpler models,
teaching us that more complexity doesn't always mean better results."
```

---

## ‚úÖ Requirement 4.4: Conclusions

### Overall Project Conclusions

**Technical Learnings:**
1. **Data Quality Matters Most** - 80% of time spent on cleaning, 20% on modeling
2. **Simple Models Often Win** - XGBoost beat complex LSTM for electricity
3. **AutoML is Powerful** - PyCaret saved hours of manual tuning
4. **Cloud Has Benefits** - AWS SageMaker scales beyond local compute

**Model Selection Insights:**
- **Regression Tasks:** PyCaret AutoML + tuning = best approach
- **Time Series:** Feature engineering + XGBoost beats deep learning
- **Production:** Simple, fast models preferred over complex, slow ones

**Deployment Learnings:**
- Streamlit enables rapid prototyping
- Docker ensures consistent environments
- GitHub Actions automates ML pipelines
- Multiple hosting options available (cloud, local, free)

**Recommendations:**
1. Start with simple baseline models
2. Use AutoML to find best algorithm family
3. Tune best model with domain knowledge
4. Deploy simplest model that meets accuracy requirements
5. Automate retraining pipelines from day one

**What Worked Well:**
- ‚úÖ Systematic notebook structure
- ‚úÖ Comprehensive EDA before modeling
- ‚úÖ Testing multiple approaches
- ‚úÖ Complete deployment pipeline

**What We'd Improve:**
- More feature engineering experimentation
- Hyperparameter tuning with Optuna
- Model ensemble exploration
- Real-time prediction API

**Business Impact:**
- **Housing Model:** Helps buyers estimate fair prices, reduces negotiation uncertainty
- **Electricity Model:** Enables grid operators to plan capacity, reduce costs

---

## ‚úÖ Requirement 4.5: Prepare for Oral Exam Questions

### Expect Questions On Your Notebooks

**Abdul's Notebooks (Dataset 2):**
```
Q: "Explain how your XGBoost model works for time series forecasting."
A: "XGBoost is a gradient boosting algorithm that builds trees sequentially.
    For time series, I created temporal features like hour, day of week, month,
    and lagged values. The model learns patterns like 'Monday mornings have 
    higher demand' and 'winter months use more electricity.' It achieved 3% 
    MAPE, our best result."

Q: "What is LSTM and why did you use it?"
A: "LSTM is a Recurrent Neural Network that remembers long sequences. I used 
    it because electricity demand has complex patterns over hours, days, and 
    seasons. It processes data sequentially and maintains 'memory' of past 
    values. However, it only achieved 7% MAPE, so XGBoost was actually better 
    for this task despite being simpler."

Q: "What does your ensemble model do?"
A: "It combines predictions from Prophet, XGBoost, and LSTM using weighted 
    averaging. Each model has different strengths - Prophet captures seasonality,
    XGBoost handles non-linear relationships, LSTM learns sequences. The 
    ensemble balances them to achieve 4% MAPE and more stable predictions."
```

**Jo's Notebooks (Dataset 1):**
```
Q: "How does PyCaret work?"
A: "PyCaret automates the ML workflow. It tests 15+ regression algorithms 
    (Linear, Ridge, Lasso, Random Forest, XGBoost, etc.), compares them with 
    cross-validation, and ranks by performance. It saved hours of manual work 
    and found the best algorithm automatically."

Q: "What's the difference between Ridge and regular Linear Regression?"
A: "Ridge adds L2 regularization - it penalizes large coefficients. This 
    prevents overfitting when features are correlated (like our economic 
    indicators). The alpha parameter controls regularization strength. Higher 
    alpha means more regularization, simpler model."
```

### Related Topics Questions

**Tree-Based Models:**
```
Q: "You used XGBoost. What other tree-based models are there?"
A: "Random Forest (builds multiple trees in parallel and averages them),
    Decision Trees (single tree, prone to overfitting),
    LightGBM (faster than XGBoost, uses histogram-based learning),
    CatBoost (handles categorical features well),
    Gradient Boosting (original boosting, slower than XGBoost)."
```

**Time Series Models:**
```
Q: "What other time series models could you have used?"
A: "ARIMA (autoregressive integrated moving average - classical approach),
    SARIMA (seasonal ARIMA),
    ETS (exponential smoothing),
    VAR (vector autoregression for multivariate),
    Transformer models (attention-based, very modern)."
```

**Metrics:**
```
Q: "Why use MAPE instead of RMSE?"
A: "MAPE (Mean Absolute Percentage Error) is scale-independent - 3% error 
    is meaningful regardless of units. RMSE penalizes large errors more but 
    depends on scale. For electricity demand (GW range), MAPE makes more 
    business sense."
```

**Deployment:**
```
Q: "Why use Docker?"
A: "Docker ensures consistent environments. The app runs identically on my 
    laptop, cloud server, or teammate's machine. It packages Python, libraries,
    code, and data into one container. No more 'works on my machine' problems."

Q: "How does your CI/CD pipeline work?"
A: "GitHub Actions monitors the repository. When code is pushed to main branch,
    it triggers a workflow that: (1) pulls latest code, (2) installs dependencies,
    (3) retrains models on new data, (4) saves updated models, (5) commits them 
    back. This automates the ML lifecycle - models stay fresh as data updates."
```

---

# 5. TESTING PROCEDURES

## 5.1 Pre-Submission Testing Checklist

### Test 1: Verify All Notebooks Run
```powershell
# Dataset 1 - Run in order
cd Dataset_1_UK_Housing/Code
# Open each 00-10 in VS Code and "Run All Cells"
# Check for errors, verify outputs appear

# Dataset 2 - Run in order
cd Dataset_2_UK_Historic_Electricity_Demand_Data/Code
# Open each 00-07 in VS Code and "Run All Cells"
# Check for errors, verify outputs appear
```

**Expected Results:**
- ‚úÖ All cells execute without errors
- ‚úÖ Outputs display (tables, charts, metrics)
- ‚úÖ Model files are created
- ‚úÖ Final datasets are produced

---

### Test 2: Verify Streamlit Apps Work
```powershell
# Test Housing App
cd Dataset_1_UK_Housing/Code
streamlit run streamlit_app.py

# In browser (http://localhost:8501):
# 1. Enter property details
# 2. Click "Predict Price"
# 3. Verify prediction appears
# 4. Test different inputs
# 5. Download CSV

# Test Electricity App
cd Dataset_2_UK_Historic_Electricity_Demand_Data/Code
streamlit run streamlit_app.py

# In browser (http://localhost:8502):
# 1. Select model
# 2. Adjust forecast horizon
# 3. Generate forecast
# 4. Verify charts display
# 5. Download CSV
```

**Expected Results:**
- ‚úÖ Apps load without errors
- ‚úÖ Predictions generate correctly
- ‚úÖ Charts render properly
- ‚úÖ Downloads work

---

### Test 3: Verify Docker Deployment
```powershell
# Build and run
docker-compose up --build

# Access apps
# Housing: http://localhost:8501
# Electricity: http://localhost:8502

# Test functionality (same as Test 2)

# Check health
docker ps

# View logs
docker-compose logs

# Stop
docker-compose down
```

**Expected Results:**
- ‚úÖ Containers build successfully
- ‚úÖ Apps accessible at ports
- ‚úÖ Health checks pass
- ‚úÖ No error logs

---

### Test 4: Verify GitHub Pipeline
```powershell
# Make minor change
echo "# Test" >> README.md

# Commit and push
git add README.md
git commit -m "Test pipeline"
git push origin main

# Monitor:
# https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025/actions

# Verify:
# - Workflow runs
# - Models retrain
# - Auto-commit appears
```

**Expected Results:**
- ‚úÖ Workflow triggers
- ‚úÖ Jobs complete successfully
- ‚úÖ Models are updated
- ‚úÖ Auto-commit includes `[skip ci]`

---

### Test 5: Verify Documentation
```powershell
# Check README
cat README.md
# Verify: Team name, members, structure, instructions

# Check DEPLOYMENT
cat DEPLOYMENT.md
# Verify: Complete deployment guide, all hosting options

# Check this file
cat PROJECT_REQUIREMENTS_CHECKLIST.md
# Verify: All requirements documented
```

**Expected Results:**
- ‚úÖ README complete and formatted
- ‚úÖ DEPLOYMENT guide comprehensive
- ‚úÖ Checklist covers all requirements

---

### Test 6: Verify Git Repository
```powershell
# Check status
git status
# Should be clean or show only intentional changes

# Check .gitignore
cat .gitignore
# Verify large files excluded

# Check for large files
git ls-files --others --ignored --exclude-standard
# Verify no large .pkl files staged

# Check remote
git remote -v
# Verify: https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025.git
```

**Expected Results:**
- ‚úÖ No large files staged
- ‚úÖ .gitignore configured correctly
- ‚úÖ Remote points to correct repository

---

## 5.2 AWS SageMaker Testing (When Instances Launch)

### Dataset 1: Linear Learner
```powershell
# 1. AWS Console ‚Üí SageMaker ‚Üí Notebook Instances
# 2. Create Instance:
#    Name: HousingPriceModel
#    Instance: ml.m4.xlarge
#    Platform: notebook-al2-v3
# 3. Wait for Status: InService (~5 min)
# 4. Open Jupyter
# 5. Upload:
#    - 09_AWS_SageMaker_Model.ipynb
#    - housing_features_final.parquet
# 6. Run all cells
# 7. Verify:
#    ‚úÖ Data uploads to S3
#    ‚úÖ Training job completes (~10 min)
#    ‚úÖ Endpoint deploys successfully
#    ‚úÖ Predictions work
# 8. Download notebook (File ‚Üí Download)
# 9. ‚ö†Ô∏è DELETE ENDPOINT (stop charges)
# 10. ‚ö†Ô∏è STOP INSTANCE (stop charges)
```

### Dataset 2: DeepAR
```powershell
# Same process as Dataset 1:
# 1. Create instance: ElectricityForecast
# 2. Upload 07_AWS_SageMaker_Model.ipynb
# 3. Upload neso_historic_demand_combined.csv
# 4. Run all cells (~15 min training)
# 5. Download completed notebook
# 6. ‚ö†Ô∏è DELETE ENDPOINT
# 7. ‚ö†Ô∏è STOP INSTANCE
```

**Expected Results:**
- ‚úÖ Training completes without errors
- ‚úÖ Metrics display (RMSE, MAE)
- ‚úÖ Endpoint responds to predictions
- ‚úÖ Notebook downloaded with outputs
- ‚úÖ Resources cleaned up (no ongoing charges)

---

## 5.3 Final Verification Before Submission

```powershell
# 1. Pull latest from remote
git pull origin main

# 2. Run complete test suite
# - Test 1: All notebooks run
# - Test 2: Streamlit apps work
# - Test 3: Docker builds and runs
# - Test 4: Pipeline triggers
# - Test 5: Documentation complete
# - Test 6: Repository clean

# 3. Verify file structure
ls -R
# Should match structure in README

# 4. Check all notebooks have authors
grep -r "Author:" Dataset_1_UK_Housing/Code/*.ipynb
grep -r "Author:" Dataset_2_UK_Historic_Electricity_Demand_Data/Code/*.ipynb
# All should show author names

# 5. Verify comparison notebooks have conclusions
code Dataset_1_UK_Housing/Code/10_final_model_comparison.ipynb
code Dataset_2_UK_Historic_Electricity_Demand_Data/Code/06_final_model_comparison.ipynb
# Check for conclusions sections

# 6. Final commit
git status
git add .
git commit -m "Final submission - All requirements complete"
git push origin main

# 7. Verify on GitHub
# https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025
# - All files visible
# - README renders correctly
# - Recent commits show
```

---

# 6. FINAL SUBMISSION CHECKLIST

## Building Models ‚úÖ

### Dataset 1: UK Housing
- [x] Quick first model (Ridge)
- [x] PyCaret AutoML
- [x] Tuned custom model (PyCaret best)
- [ ] AWS SageMaker model (template ready, needs AWS execution)
- [x] Model comparison notebook with metrics
- [x] Conclusions on best model

### Dataset 2: UK Electricity
- [x] Quick first model (Naive, Moving Average)
- [x] PyCaret AutoML (Section 5 of complete training)
- [x] Tuned custom models (Prophet, XGBoost, LSTM, Ensemble - 4 models!)
- [ ] AWS SageMaker model (template ready, needs AWS execution)
- [x] Model comparison notebook with metrics
- [x] Conclusions (XGBoost best with 3% MAPE)

---

## Deployment ‚úÖ

- [x] Frontend: 2 Streamlit apps (Housing + Electricity)
- [x] Backend: Integrated in Streamlit (model loading, predictions)
- [x] Pipeline: GitHub Actions CI/CD (automated retraining)
- [x] Hosting: Multiple options documented (Docker, Oracle, AWS, Pi, Streamlit Cloud)

---

## Upload Requirements ‚úÖ

- [x] EDA notebooks with cleaning & graphs (10 total across both datasets)
- [x] Final import notebooks (1 per dataset)
- [x] Model notebooks (1 per model - 10+ total)
- [x] Model outputs preserved (for long-running models)
- [x] Large .pkl files excluded from git (< 100MB rule)
- [ ] AWS notebooks downloaded with outputs (pending AWS execution)
- [x] Comparison notebooks with deep analysis and conclusions (2 total)

---

## Presentation Preparation ‚úÖ

- [x] Who's who prepared (Team name, members, roles)
- [x] EDA findings documented (expected & unexpected)
- [x] Model comparison ready (easiest vs best vs most practical)
- [x] Conclusions documented (learnings, recommendations)
- [x] Oral exam Q&A prepared (XGBoost, LSTM, tree models, time series)
- [x] Code explanations ready (able to explain any notebook)

---

## Documentation ‚úÖ

- [x] README.md (team, structure, setup, instructions)
- [x] DEPLOYMENT.md (complete hosting guide)
- [x] PROJECT_REQUIREMENTS_CHECKLIST.md (this file)
- [x] Author attribution in ALL notebooks
- [x] Markdown explanations in notebooks
- [x] .gitignore configured correctly
- [x] .dockerignore for optimized builds

---

## Testing ‚úÖ

- [x] Notebooks run without errors
- [x] Streamlit apps tested locally
- [x] Docker deployment verified
- [x] GitHub pipeline tested
- [x] Documentation reviewed
- [ ] AWS SageMaker notebooks tested (pending instance launch)

---

## Final Steps (Before 11:59 PM Nov 24)

1. [ ] Run AWS SageMaker training for both datasets
2. [ ] Download completed AWS notebooks with outputs
3. [ ] Replace template notebooks with completed versions
4. [ ] Update model comparison notebooks with AWS metrics
5. [ ] Run final test suite (all 6 tests)
6. [ ] Final git commit and push
7. [ ] Verify repository on GitHub web interface
8. [ ] Submit Canvas link: https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025
9. [ ] Prepare presentation slides (optional but recommended)
10. [ ] Review this checklist with team

---

## Known Limitations & Notes

**AWS Models:**
- Templates are complete and ready
- Waiting for AWS instance to become InService
- Expected training time: 10-15 min (Housing), 15-20 min (Electricity)
- Remember to DELETE endpoints and STOP instances after downloading!

**Model Files:**
- Large .pkl files excluded from git (assignment requirement)
- Training outputs preserved in notebooks (visible)
- Small pipeline models committed for CI/CD

**Streamlit Apps:**
- Fully functional locally
- Can deploy to Streamlit Cloud immediately
- Docker containers ready for any server

**GitHub Pipeline:**
- Automated retraining on push
- Uses `[skip ci]` to prevent infinite loops
- Retrains simplified models (Ridge, XGBoost) for speed

---

## Emergency Contact & Backup

**If AWS Fails:**
- All other requirements are complete
- Local PyCaret and custom models demonstrate full capability
- AWS templates show understanding of cloud deployment
- Can explain approach in presentation

**If Deployment Fails:**
- Docker-compose.yml ready
- DEPLOYMENT.md has 5 different hosting options
- Can demonstrate locally during presentation
- Screenshots can be prepared as backup

**If Git Issues:**
- Repository is: https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025
- All files are tracked and committed
- .gitignore configured correctly
- Can re-clone from GitHub if needed

---

## Presentation Day Checklist (Nov 28)

**Bring:**
- [ ] Laptop with repository cloned
- [ ] Backup USB drive with repository
- [ ] Screenshots of key results
- [ ] Notebook outputs (PDF exports)
- [ ] This checklist (for Q&A reference)

**Prepare:**
- [ ] Review EDA findings
- [ ] Review model results
- [ ] Practice explaining XGBoost, LSTM, ensemble
- [ ] Review deployment architecture (Docker, GitHub Actions)
- [ ] Prepare answers for tree-based models question
- [ ] Prepare answers for time series models question

**Test Before Leaving:**
- [ ] Streamlit apps run locally
- [ ] Can access GitHub repository
- [ ] Notebooks open in VS Code
- [ ] Have backup screenshots

---

**Document Version:** 1.0  
**Last Updated:** November 24, 2025  
**Status:** READY FOR SUBMISSION (pending AWS execution)

**Created by:** Abdul Salam Aldabik  
**Team:** CloudAI Analytics Team  
**Repository:** https://github.com/JoNaulaerts/Machine-Learning-Project-TM-2025
