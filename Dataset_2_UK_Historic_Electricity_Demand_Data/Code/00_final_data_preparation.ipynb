{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e38f494",
   "metadata": {},
   "source": [
    "# UK Historic Electricity Demand - Final Data Preparation\n",
    "**Author:** Abdul Salam Aldabik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")\n",
    "print(f\"üìÖ Processing date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98529ac0",
   "metadata": {},
   "source": [
    "## 1. Load Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56814d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined dataset (created by data_pull.ipynb)\n",
    "print(\"üìÇ Loading electricity demand data...\")\n",
    "df = pd.read_csv('../Data/neso_historic_demand_combined.csv')\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded\")\n",
    "print(f\"   Rows: {len(df):,}\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e5e6e",
   "metadata": {},
   "source": [
    "## 2. Clean Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names: lowercase, no spaces\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "print(f\"‚úÖ Column names standardized\")\n",
    "print(f\"   Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a4598",
   "metadata": {},
   "source": [
    "## 3. Remove Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16319406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns (nd, tsd are duplicates of england_wales_demand)\n",
    "# Drop near-empty columns (interconnector flows with >90% missing data)\n",
    "# Protect settlement_date and settlement_period (critical for time series)\n",
    "\n",
    "redundant = ['nd', 'tsd']\n",
    "near_empty = ['nsl_flow', 'eleclink_flow', 'viking_flow', 'greenlink_flow', 'scottish_transfer']\n",
    "\n",
    "# Find constant columns (single value) but protect temporal columns\n",
    "constant_cols = [col for col in df.columns \n",
    "                 if df[col].nunique(dropna=True) <= 1 \n",
    "                 and col not in ['settlement_date', 'settlement_period']]\n",
    "\n",
    "cols_to_drop = list(set(redundant + near_empty + constant_cols))\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "print(f\"‚úÖ Dropped {len(cols_to_drop)} redundant/empty columns\")\n",
    "print(f\"   Remaining: {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e83b51",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing settlement_date (critical for time series - cannot be fabricated)\n",
    "if 'settlement_date' in df.columns:\n",
    "    missing_dates = df['settlement_date'].isna().sum()\n",
    "    if missing_dates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Dropping {missing_dates:,} rows with missing settlement_date\")\n",
    "        df = df.dropna(subset=['settlement_date'])\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df['settlement_date'] = pd.to_datetime(df['settlement_date'], errors='coerce')\n",
    "    \n",
    "    # Sort chronologically\n",
    "    df = df.sort_values(['settlement_date', 'settlement_period'], ignore_index=True)\n",
    "    print(f\"‚úÖ Data sorted chronologically\")\n",
    "\n",
    "# Forward fill numeric columns (time series best practice)\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_cols:\n",
    "    missing_before = df[col].isnull().sum()\n",
    "    if missing_before > 0:\n",
    "        df[col] = df[col].ffill().bfill()  # Forward then backward fill\n",
    "        print(f\"   {col}: filled {missing_before:,} missing values\")\n",
    "\n",
    "print(f\"‚úÖ Missing values handled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb7259",
   "metadata": {},
   "source": [
    "## 5. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8218d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap outliers at 0.5th and 99.5th percentiles (keeps extreme values but removes statistical outliers)\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col not in ['settlement_period', 'year', 'month', 'day']:  # Don't cap temporal features\n",
    "        lower = df[col].quantile(0.005)\n",
    "        upper = df[col].quantile(0.995)\n",
    "        df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "print(f\"‚úÖ Outliers capped for {len(numeric_cols)} numeric columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68bc08",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering - Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive temporal features from settlement_date\n",
    "if 'settlement_date' in df.columns:\n",
    "    df['settlement_date'] = pd.to_datetime(df['settlement_date'])\n",
    "    \n",
    "    # Year-level features\n",
    "    df['year'] = df['settlement_date'].dt.year\n",
    "    df['quarter'] = df['settlement_date'].dt.quarter\n",
    "    \n",
    "    # Month and day features\n",
    "    df['month'] = df['settlement_date'].dt.month\n",
    "    df['day'] = df['settlement_date'].dt.day\n",
    "    df['day_of_year'] = df['settlement_date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['settlement_date'].dt.isocalendar().week\n",
    "    \n",
    "    # Weekday features\n",
    "    df['day_of_week'] = df['settlement_date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Hour from settlement_period (1-48 half-hourly periods)\n",
    "    df['hour'] = (df['settlement_period'] - 1) * 0.5\n",
    "    \n",
    "    temporal_features = ['year', 'quarter', 'month', 'day', 'day_of_week', \n",
    "                        'day_of_year', 'week_of_year', 'is_weekend', 'hour']\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(temporal_features)} temporal features\")\n",
    "    for feat in ['year', 'month', 'hour']:\n",
    "        print(f\"   {feat}: {df[feat].min()} to {df[feat].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2238926",
   "metadata": {},
   "source": [
    "## 7. Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7da1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with correlation >0.95 (but protect temporal features)\n",
    "temporal_cols = ['settlement_date', 'settlement_period', 'year', 'quarter', 'month', \n",
    "                'day', 'day_of_week', 'day_of_year', 'week_of_year', 'is_weekend', 'hour']\n",
    "\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "corr_matrix = numeric_df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop_corr = [column for column in upper.columns \n",
    "                if any(upper[column] > 0.95) \n",
    "                and column not in temporal_cols]\n",
    "\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(f\"‚úÖ Dropped {len(to_drop_corr)} highly correlated columns\")\n",
    "print(f\"   Final columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82db075",
   "metadata": {},
   "source": [
    "## 8. Final Validation & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nShape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Verify date completeness\n",
    "if 'settlement_date' in df.columns:\n",
    "    print(f\"\\nDate range: {df['settlement_date'].min()} to {df['settlement_date'].max()}\")\n",
    "    print(f\"Unique dates: {df['settlement_date'].nunique():,}\")\n",
    "    print(f\"Missing dates: {df['settlement_date'].isna().sum()}\")\n",
    "\n",
    "# Verify no missing values\n",
    "missing_total = df.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values: {missing_total}\")\n",
    "\n",
    "if missing_total == 0:\n",
    "    print(\"‚úÖ Dataset ready for modeling!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Still have {missing_total} missing values\")\n",
    "\n",
    "# List key features\n",
    "temporal_features = [col for col in df.columns if col in temporal_cols]\n",
    "demand_features = [col for col in df.columns if 'demand' in col.lower() or 'generation' in col.lower()]\n",
    "\n",
    "print(f\"\\nTemporal features ({len(temporal_features)}): {temporal_features}\")\n",
    "print(f\"Demand/generation features ({len(demand_features)}): {demand_features}\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "output_path = '../Data/cleaned_and_augmented_electricity_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ SAVED: {output_path}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518e79c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Data Preparation Complete:**\n",
    "- ‚úÖ Loaded and combined all years (2001-2025)\n",
    "- ‚úÖ Cleaned column names\n",
    "- ‚úÖ Removed redundant/empty columns\n",
    "- ‚úÖ Handled missing values with forward fill\n",
    "- ‚úÖ Capped outliers\n",
    "- ‚úÖ Created 9 temporal features\n",
    "- ‚úÖ Removed multicollinear features\n",
    "- ‚úÖ Saved clean dataset\n",
    "\n",
    "**Next Steps:**\n",
    "- Run model training notebooks (07_complete_model_training.ipynb)\n",
    "- Deploy models via Streamlit (streamlit_app.py)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
