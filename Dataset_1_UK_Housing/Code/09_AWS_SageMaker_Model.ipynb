{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d0d405",
   "metadata": {},
   "source": [
    "# UK Housing Price Prediction - AWS SageMaker Linear Learner\n",
    "**Author:** Abdul Salam Aldabik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf1320",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ffdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (only if not already installed in SageMaker)\n",
    "# !pip install sagemaker boto3 pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import io\n",
    "import os\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb984f",
   "metadata": {},
   "source": [
    "## 2. SageMaker Session and Role Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416adb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker session and get execution role\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()  # Default S3 bucket for this session\n",
    "prefix = 'housing-price-prediction'  # S3 prefix for organizing data\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"S3 Prefix: {prefix}\")\n",
    "print(f\"Region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ba21c",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data\n",
    "\n",
    "**NOTE:** Before running this cell, you need to upload your `housing_features_final.parquet` file to SageMaker.\n",
    "\n",
    "Options:\n",
    "1. Upload via JupyterLab interface (drag & drop)\n",
    "2. Use AWS S3 console to upload, then download in notebook\n",
    "3. Use boto3 to download from your S3 bucket if you've already uploaded it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ab928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from local file (if you uploaded it to SageMaker)\n",
    "# df = pd.read_parquet('housing_features_final.parquet')\n",
    "\n",
    "# Option 2: Download from S3 if you've already uploaded it\n",
    "# s3 = boto3.client('s3')\n",
    "# s3.download_file('your-bucket-name', 'path/to/housing_features_final.parquet', 'housing_features_final.parquet')\n",
    "# df = pd.read_parquet('housing_features_final.parquet')\n",
    "\n",
    "# For now, using placeholder - REPLACE THIS with actual data loading\n",
    "print(\"⚠️ IMPORTANT: Load your housing_features_final.parquet file here\")\n",
    "print(\"Uncomment one of the options above and run this cell\")\n",
    "\n",
    "# Example - Replace with your actual file\n",
    "df = pd.read_parquet('housing_features_final.parquet')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f92ef",
   "metadata": {},
   "source": [
    "## 4. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "target = 'log_price'  # or 'price' depending on your setup\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [col for col in ['log_price', 'price', 'date_of_transfer', 'transaction_unique_identifier'] \n",
    "             if col in df.columns]\n",
    "\n",
    "# Get feature columns\n",
    "features = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Features: {features[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any Period or non-numeric columns\n",
    "period_cols = [col for col in df.columns if pd.api.types.is_period_dtype(df[col])]\n",
    "if period_cols:\n",
    "    print(f\"Converting Period columns: {period_cols}\")\n",
    "    for col in period_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "# Convert any remaining object columns to numeric or drop them\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Ensure all features are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        print(f\"Converting {col} to numeric...\")\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Fill any NaN values\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nFinal X shape: {X.shape}\")\n",
    "print(f\"Final y shape: {y.shape}\")\n",
    "print(f\"\\nData types:\\n{X.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5628df",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed94997",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for SageMaker Linear Learner\n",
    "\n",
    "SageMaker Linear Learner expects:\n",
    "- CSV format with target in first column\n",
    "- No header row\n",
    "- Data uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine target and features for SageMaker format (target first)\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV without header and index\n",
    "train_file = 'housing_train.csv'\n",
    "test_file = 'housing_test.csv'\n",
    "\n",
    "train_data.to_csv(train_file, header=False, index=False)\n",
    "test_data.to_csv(test_file, header=False, index=False)\n",
    "\n",
    "print(f\"Saved {train_file} ({os.path.getsize(train_file) / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"Saved {test_file} ({os.path.getsize(test_file) / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400b8d4",
   "metadata": {},
   "source": [
    "## 7. Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data to S3\n",
    "train_s3_path = sess.upload_data(\n",
    "    path=train_file,\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/train'\n",
    ")\n",
    "\n",
    "# Upload test data to S3\n",
    "test_s3_path = sess.upload_data(\n",
    "    path=test_file,\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/test'\n",
    ")\n",
    "\n",
    "print(f\"Training data uploaded to: {train_s3_path}\")\n",
    "print(f\"Test data uploaded to: {test_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf1348",
   "metadata": {},
   "source": [
    "## 8. Configure SageMaker Linear Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daecbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Linear Learner container image\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve('linear-learner', sess.boto_region_name, version='latest')\n",
    "print(f\"Linear Learner container: {container}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Learner estimator\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',  # As per lab guide\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "linear.set_hyperparameters(\n",
    "    feature_dim=X_train.shape[1],  # Number of features\n",
    "    predictor_type='regressor',     # For regression (price prediction)\n",
    "    mini_batch_size=100,\n",
    "    epochs=10,\n",
    "    learning_rate=0.01,\n",
    "    normalize_data=True,\n",
    "    normalize_label=True\n",
    ")\n",
    "\n",
    "print(\"Linear Learner estimator configured\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb8dbc",
   "metadata": {},
   "source": [
    "## 9. Train the Model\n",
    "\n",
    "**⚠️ This will take 5-10 minutes and incur AWS charges!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training input channels\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=train_s3_path,\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "test_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=test_s3_path,\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training job...\")\n",
    "print(\"This will take approximately 5-10 minutes.\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "linear.fit({'train': train_input, 'test': test_input})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30453646",
   "metadata": {},
   "source": [
    "## 10. Deploy the Model\n",
    "\n",
    "**⚠️ Deploying creates an endpoint that incurs hourly charges. Remember to delete it when done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to an endpoint\n",
    "print(\"Deploying model to endpoint...\")\n",
    "print(\"This will take 5-10 minutes.\")\n",
    "\n",
    "predictor = linear.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1ff6b",
   "metadata": {},
   "source": [
    "## 11. Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict in batches\n",
    "def predict_batches(predictor, X, batch_size=500):\n",
    "    \"\"\"Predict in batches to avoid timeout issues\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch = X.iloc[i:i+batch_size]\n",
    "        result = predictor.predict(batch.values)\n",
    "        \n",
    "        # Extract predictions from response\n",
    "        batch_preds = [pred['score'] for pred in result['predictions']]\n",
    "        predictions.extend(batch_preds)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + len(batch)}/{len(X)} samples...\")\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred = predict_batches(predictor, X_test)\n",
    "print(f\"\\nPredictions shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AWS SageMaker Linear Learner - Model Performance\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Squared Error (MSE):     {mse:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):     {mae:,.2f}\")\n",
    "print(f\"R² Score:                      {r2:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# If using log_price, show actual price RMSE\n",
    "if target == 'log_price':\n",
    "    actual_prices = np.exp(y_test)\n",
    "    predicted_prices = np.exp(y_pred)\n",
    "    actual_rmse = np.sqrt(mean_squared_error(actual_prices, predicted_prices))\n",
    "    print(f\"\\nActual Price RMSE: £{actual_rmse:,.2f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a850963",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.3, s=1)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f'Predicted vs Actual (R² = {r2:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.3, s=1)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d4218",
   "metadata": {},
   "source": [
    "## 13. Save Results for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to file for comparison notebook\n",
    "results = {\n",
    "    'model': 'AWS SageMaker Linear Learner',\n",
    "    'mse': mse,\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'r2': r2,\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': X_train.shape[1],\n",
    "    'endpoint_name': predictor.endpoint_name\n",
    "}\n",
    "\n",
    "if target == 'log_price':\n",
    "    results['actual_price_rmse'] = actual_rmse\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('aws_sagemaker_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to aws_sagemaker_results.json\")\n",
    "print(\"\\nResults:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf34b4f",
   "metadata": {},
   "source": [
    "## 14. ⚠️ IMPORTANT: Clean Up Resources\n",
    "\n",
    "**Delete the endpoint to stop incurring charges!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb733cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {predictor.endpoint_name}\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"✅ Endpoint deleted successfully!\")\n",
    "print(\"\\n⚠️ Remember to also stop your notebook instance in the SageMaker console!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dea20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ✅ Loaded and prepared UK Housing data\n",
    "2. ✅ Uploaded data to S3\n",
    "3. ✅ Trained AWS SageMaker Linear Learner model\n",
    "4. ✅ Deployed model to endpoint\n",
    "5. ✅ Made predictions and evaluated performance\n",
    "6. ✅ Saved results for comparison\n",
    "7. ✅ Cleaned up resources\n",
    "\n",
    "**Next Steps:**\n",
    "- Download this notebook from SageMaker\n",
    "- Save it to your GitHub repo as `09_AWS_SageMaker_Model.ipynb`\n",
    "- Use the results in your model comparison notebook\n",
    "- Remember to stop your SageMaker notebook instance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
