{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9ef803",
   "metadata": {},
   "source": [
    "# UK Housing Price Prediction - Final Model Comparison\n",
    "**Author:** Abdul Salam Aldabik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553f1b9",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fa4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436fbaf",
   "metadata": {},
   "source": [
    "## 2. Collect Model Results\n",
    "\n",
    "**Note:** These results should be populated from your actual model training notebooks.\n",
    "You'll need to:\n",
    "1. Run each model notebook\n",
    "2. Copy the metrics here\n",
    "3. Or load from saved JSON files if you saved them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model results - UPDATE THESE with your actual results!\n",
    "\n",
    "# Example structure - replace with your actual metrics\n",
    "results = {\n",
    "    'First Simple Model (Ridge)': {\n",
    "        'mse': 0.0,  # UPDATE from notebook 06\n",
    "        'rmse': 0.0,\n",
    "        'mae': 0.0,\n",
    "        'r2': 0.0,\n",
    "        'training_time': 'N/A',\n",
    "        'complexity': 'Low',\n",
    "        'description': 'Simple Ridge regression baseline'\n",
    "    },\n",
    "    'PyCaret AutoML': {\n",
    "        'mse': 0.0,  # UPDATE from notebook 07\n",
    "        'rmse': 0.0,\n",
    "        'mae': 0.0,\n",
    "        'r2': 0.0,\n",
    "        'best_model': 'Unknown',  # e.g., 'XGBoost', 'LightGBM'\n",
    "        'training_time': 'N/A',\n",
    "        'complexity': 'Medium',\n",
    "        'description': 'Automated ML with model selection'\n",
    "    },\n",
    "    'AWS SageMaker Linear Learner': {\n",
    "        'mse': 0.0,  # UPDATE from notebook 09 or AWS results\n",
    "        'rmse': 0.0,\n",
    "        'mae': 0.0,\n",
    "        'r2': 0.0,\n",
    "        'training_time': '~5-10 min',\n",
    "        'complexity': 'High (Cloud)',\n",
    "        'description': 'AWS managed Linear Learner algorithm'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model results structure created.\")\n",
    "print(\"‚ö†Ô∏è Remember to update with actual metrics from your notebooks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load results from JSON files if you saved them\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "# try:\n",
    "#     with open('simple_model_results.json', 'r') as f:\n",
    "#         simple_results = json.load(f)\n",
    "#         results['First Simple Model (Ridge)'].update(simple_results)\n",
    "#     print(\"‚úÖ Loaded simple model results\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"‚ö†Ô∏è simple_model_results.json not found\")\n",
    "\n",
    "# try:\n",
    "#     with open('pycaret_results.json', 'r') as f:\n",
    "#         pycaret_results = json.load(f)\n",
    "#         results['PyCaret AutoML'].update(pycaret_results)\n",
    "#     print(\"‚úÖ Loaded PyCaret results\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"‚ö†Ô∏è pycaret_results.json not found\")\n",
    "\n",
    "# try:\n",
    "#     with open('aws_sagemaker_results.json', 'r') as f:\n",
    "#         aws_results = json.load(f)\n",
    "#         results['AWS SageMaker Linear Learner'].update(aws_results)\n",
    "#     print(\"‚úÖ Loaded AWS SageMaker results\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"‚ö†Ô∏è aws_sagemaker_results.json not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96d9a0",
   "metadata": {},
   "source": [
    "## 3. Create Comparison DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': metrics.get('rmse', 0),\n",
    "        'MAE': metrics.get('mae', 0),\n",
    "        'R¬≤': metrics.get('r2', 0),\n",
    "        'MSE': metrics.get('mse', 0),\n",
    "        'Complexity': metrics.get('complexity', 'N/A'),\n",
    "        'Training Time': metrics.get('training_time', 'N/A')\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcfc92",
   "metadata": {},
   "source": [
    "## 4. Visualize Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfd1ff",
   "metadata": {},
   "source": [
    "### 4.1 RMSE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a330d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = ax.bar(df_comparison['Model'], df_comparison['RMSE'], color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Root Mean Squared Error Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_rmse_idx = df_comparison['RMSE'].idxmin()\n",
    "best_rmse_model = df_comparison.loc[best_rmse_idx, 'Model']\n",
    "best_rmse_value = df_comparison.loc[best_rmse_idx, 'RMSE']\n",
    "print(f\"\\nüèÜ Best RMSE: {best_rmse_model} with RMSE = {best_rmse_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e014d0",
   "metadata": {},
   "source": [
    "### 4.2 R¬≤ Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(df_comparison['Model'], df_comparison['R¬≤'], color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('R¬≤ Score (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('R¬≤ Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_r2_idx = df_comparison['R¬≤'].idxmax()\n",
    "best_r2_model = df_comparison.loc[best_r2_idx, 'Model']\n",
    "best_r2_value = df_comparison.loc[best_r2_idx, 'R¬≤']\n",
    "print(f\"\\nüèÜ Best R¬≤: {best_r2_model} with R¬≤ = {best_r2_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0618a",
   "metadata": {},
   "source": [
    "### 4.3 MAE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(df_comparison['Model'], df_comparison['MAE'], color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('MAE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Mean Absolute Error Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_mae_idx = df_comparison['MAE'].idxmin()\n",
    "best_mae_model = df_comparison.loc[best_mae_idx, 'Model']\n",
    "best_mae_value = df_comparison.loc[best_mae_idx, 'MAE']\n",
    "print(f\"\\nüèÜ Best MAE: {best_mae_model} with MAE = {best_mae_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a6c39",
   "metadata": {},
   "source": [
    "### 4.4 Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431be4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize metrics for comparison (0-1 scale)\n",
    "df_normalized = df_comparison.copy()\n",
    "\n",
    "# For RMSE and MAE: lower is better, so we invert\n",
    "df_normalized['RMSE_norm'] = 1 - (df_normalized['RMSE'] - df_normalized['RMSE'].min()) / (df_normalized['RMSE'].max() - df_normalized['RMSE'].min() + 1e-10)\n",
    "df_normalized['MAE_norm'] = 1 - (df_normalized['MAE'] - df_normalized['MAE'].min()) / (df_normalized['MAE'].max() - df_normalized['MAE'].min() + 1e-10)\n",
    "\n",
    "# For R¬≤: higher is better, so we keep as is (already 0-1)\n",
    "df_normalized['R2_norm'] = df_normalized['R¬≤']\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(df_normalized['Model']))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, df_normalized['RMSE_norm'], width, label='RMSE (normalized)', color='#3498db', alpha=0.8)\n",
    "ax.bar(x, df_normalized['MAE_norm'], width, label='MAE (normalized)', color='#e74c3c', alpha=0.8)\n",
    "ax.bar(x + width, df_normalized['R2_norm'], width, label='R¬≤ (normalized)', color='#2ecc71', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Normalized Score (1.0 = Best)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Multi-Metric Performance Comparison (Normalized)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_normalized['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c118f89",
   "metadata": {},
   "source": [
    "### 4.5 Overall Performance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b77548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall score (average of normalized metrics)\n",
    "df_normalized['Overall_Score'] = (df_normalized['RMSE_norm'] + \n",
    "                                   df_normalized['MAE_norm'] + \n",
    "                                   df_normalized['R2_norm']) / 3\n",
    "\n",
    "# Sort by overall score\n",
    "df_normalized = df_normalized.sort_values('Overall_Score', ascending=False)\n",
    "\n",
    "# Plot overall scores\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.barh(df_normalized['Model'], df_normalized['Overall_Score'], \n",
    "               color=['#2ecc71', '#3498db', '#e74c3c'][:len(df_normalized)], alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{width:.3f}',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Overall Performance Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Overall Model Performance Ranking', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE RANKING\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in df_normalized.iterrows():\n",
    "    print(f\"{idx+1}. {row['Model']}: {row['Overall_Score']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aacad3",
   "metadata": {},
   "source": [
    "## 5. Detailed Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d96ae",
   "metadata": {},
   "source": [
    "### 5.1 Model Strengths and Weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nüìä {model_name}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Description: {metrics.get('description', 'N/A')}\")\n",
    "    print(f\"Complexity: {metrics.get('complexity', 'N/A')}\")\n",
    "    print(f\"Training Time: {metrics.get('training_time', 'N/A')}\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  - RMSE: {metrics.get('rmse', 0):.4f}\")\n",
    "    print(f\"  - MAE:  {metrics.get('mae', 0):.4f}\")\n",
    "    print(f\"  - R¬≤:   {metrics.get('r2', 0):.4f}\")\n",
    "    \n",
    "    # Add model-specific info\n",
    "    if 'best_model' in metrics:\n",
    "        print(f\"  - Best Algorithm: {metrics['best_model']}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50155e56",
   "metadata": {},
   "source": [
    "### 5.2 Model Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba69c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trade-off analysis\n",
    "tradeoffs = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'First Simple Model',\n",
    "        'Pros': 'Fast training, simple, interpretable, good baseline',\n",
    "        'Cons': 'Lower accuracy, limited feature interactions',\n",
    "        'Best For': 'Quick prototyping, baseline comparison'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'PyCaret AutoML',\n",
    "        'Pros': 'Automated, tests multiple algorithms, good accuracy',\n",
    "        'Cons': 'Longer training time, less control over process',\n",
    "        'Best For': 'Finding best algorithm quickly, production use'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'AWS SageMaker',\n",
    "        'Pros': 'Scalable, managed infrastructure, production-ready',\n",
    "        'Cons': 'Requires cloud setup, potential costs, complexity',\n",
    "        'Best For': 'Large-scale deployment, enterprise applications'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRADE-OFF ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in tradeoffs.iterrows():\n",
    "    print(f\"\\n{row['Model']}:\")\n",
    "    print(f\"  ‚úÖ Pros: {row['Pros']}\")\n",
    "    print(f\"  ‚ùå Cons: {row['Cons']}\")\n",
    "    print(f\"  üéØ Best For: {row['Best For']}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9a3ed",
   "metadata": {},
   "source": [
    "## 6. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4442d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine overall winner\n",
    "winner = df_normalized.iloc[0]['Model']\n",
    "winner_score = df_normalized.iloc[0]['Overall_Score']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ **Overall Winner**: {winner}\")\n",
    "print(f\"   Overall Performance Score: {winner_score:.4f}\")\n",
    "print(f\"\\nüìä **Key Findings:**\")\n",
    "print(f\"   - Best RMSE: {best_rmse_model} ({best_rmse_value:.4f})\")\n",
    "print(f\"   - Best R¬≤: {best_r2_model} ({best_r2_value:.4f})\")\n",
    "print(f\"   - Best MAE: {best_mae_model} ({best_mae_value:.4f})\")\n",
    "\n",
    "print(f\"\\nüí° **Recommendations:**\")\n",
    "print(f\"   1. For Production Deployment: Use {winner} for best overall performance\")\n",
    "print(f\"   2. For Quick Prototyping: Use First Simple Model for baseline\")\n",
    "print(f\"   3. For Scalability: AWS SageMaker provides best cloud infrastructure\")\n",
    "print(f\"   4. For Automation: PyCaret offers best algorithm selection process\")\n",
    "\n",
    "print(f\"\\nüìà **Next Steps:**\")\n",
    "print(f\"   - Fine-tune hyperparameters of winning model\")\n",
    "print(f\"   - Collect more data to improve predictions\")\n",
    "print(f\"   - Deploy winning model to Streamlit app\")\n",
    "print(f\"   - Monitor model performance in production\")\n",
    "print(f\"   - Consider ensemble methods combining multiple models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07f439",
   "metadata": {},
   "source": [
    "## 7. Save Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison table to CSV\n",
    "df_comparison.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"‚úÖ Comparison results saved to: model_comparison_results.csv\")\n",
    "\n",
    "# Save detailed results to JSON\n",
    "comparison_summary = {\n",
    "    'comparison_table': df_comparison.to_dict(orient='records'),\n",
    "    'winner': winner,\n",
    "    'winner_score': float(winner_score),\n",
    "    'best_rmse': {'model': best_rmse_model, 'value': float(best_rmse_value)},\n",
    "    'best_r2': {'model': best_r2_model, 'value': float(best_r2_value)},\n",
    "    'best_mae': {'model': best_mae_model, 'value': float(best_mae_value)},\n",
    "    'all_results': results\n",
    "}\n",
    "\n",
    "with open('model_comparison_summary.json', 'w') as f:\n",
    "    json.dump(comparison_summary, f, indent=2)\n",
    "print(\"‚úÖ Detailed summary saved to: model_comparison_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0505e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook compared three different approaches to UK housing price prediction:\n",
    "\n",
    "1. **First Simple Model (Ridge Regression)**: Baseline approach\n",
    "2. **PyCaret AutoML**: Automated machine learning with algorithm selection\n",
    "3. **AWS SageMaker Linear Learner**: Cloud-based scalable solution\n",
    "\n",
    "### Key Takeaways:\n",
    "- All models were evaluated on RMSE, MAE, and R¬≤ metrics\n",
    "- Trade-offs between accuracy, complexity, and deployment were considered\n",
    "- The winning model provides best overall performance for production use\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy the winning model in Streamlit application\n",
    "- Continue monitoring and improving model performance\n",
    "- Consider ensemble approaches for even better results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
